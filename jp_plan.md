# 日本語版タグリスト解析スクリプト計画

## 概要

複数のファイル (`tag-list.txt` および `fragment_*.txt`) に分割されたSCP財団Wikiの日本語版タグリストを解析し、タグ情報を抽出してJSONファイル (`jp_tags.json`) を作成するPythonスクリプトの計画です。

## 処理フロー

1.  **ファイル群の読み込みと結合:**
    *   起点となる `scp-jp/tag-list.txt` を読み込みます。
    *   ファイル内容を走査し、`[[include :scp-jp:fragment:FILE_NAME]]` 構文を見つけたら、対応するフラグメントファイル (`scp-jp/fragment_FILE_NAME.txt`) の内容を読み込み、元の位置に展開します。これを再帰的に行い、全てのファイル内容を結合した単一のテキストデータを生成します。
    *   循環参照を防止するため、既に訪れたファイルのパスを記録します。

2.  **タグ情報抽出:** 結合されたテキストデータを行ごとに走査し、正規表現を用いて以下の情報を抽出します。
    *   **セクション情報:** `+ タイトル` または `++ タイトル` 形式のセクションヘッダを検出し、カテゴリ階層を構築します。
    *   **タグ定義:** `* **[[[/system:page-tags/tag/タグスラッグ|タグ名]]]** //(英語名)// - 説明文` 形式のタグ定義行から情報を抽出します。
    *   **メタ情報:** `* // KEY: VALUE //` 形式のメタ情報行から追加情報を抽出します。

3.  **データ構造化:** 抽出した情報をタグごとにPythonの辞書オブジェクトとしてまとめます。英語タグ名はオプション情報として扱います。
    ```python
    {
        "name": "日本語タグ名",
        "slug": "タグスラッグ",
        "name_en": "英語タグ名" or None,
        "description": "説明文",
        "category": "カテゴリ名",
        "meta": { # メタ情報
            "related_tags": ["関連タグ1", "関連タグ2", ...],
            "see_also": ["参照タグ1", "参照タグ2", ...],
            "key1": ["value1", "value2"],
            ...
        }
    }
    ```

4.  **JSON出力:** 構造化されたタグデータのリストをJSONファイル (`jp_tags.json`) に書き出します。

## 実装上の課題と解決策

### 1. セクション検出の問題

**問題:** 当初の正規表現パターン `^#+\s*(.*)` はCSSルールの `#` セレクタと誤って一致していました。

**解決策:** セクションヘッダの正規表現パターンを `^\+\+?\s+(.*?)(?:\[\[#.*)?$` に変更し、`+` または `++` で始まる行のみを検出するようにしました。

### 2. タグ定義の抽出

**問題:** 当初の計画では、タグ定義の形式が `[[...|JP_TAG_NAME]]]` と想定されていましたが、実際のファイルでは `* **[[[/system:page-tags/tag/タグスラッグ|タグ名]]]** //(英語名)// - 説明文` という形式でした。

**解決策:** タグ定義の正規表現パターンを `^\*\s+\*\*\[{3}/system:page-tags/tag/([^|]+)\|([^\]]+)\]{3}\*\*\s+(?://\(([^)]+)\)//\s+)?-\s*(.*)` に変更し、タグスラッグ、タグ名、英語名（オプション）、説明文を正確に抽出できるようにしました。

### 3. 文字エンコーディングの問題

**問題:** 日本語や中国語などの非ASCII文字をコンソールに出力する際にエンコーディングエラーが発生しました。

**解決策:** `safe_print` 関数を実装し、UnicodeEncodeErrorが発生した場合にはASCIIエスケープシーケンスに変換して出力するようにしました。

## 処理フロー図 (Mermaid)

```mermaid
graph TD
    A[開始] --> B{ファイルリスト定義\n(scp-jp/tag-list.txt, scp-jp/fragment_*.txt)};
    B --> C{全ファイル内容読み込み};
    C --> D{Include展開処理\n(再帰的に全ファイルを結合)};
    D --> E{結合テキストを行ごとに処理};
    E --> S{セクションヘッダか？\n(+ または ++)};
    S -- Yes --> T{カテゴリ階層を更新};
    T --> E;
    S -- No --> F{タグ定義行か？\n(正規表現)};
    F -- Yes --> G{タグスラッグ, タグ名, 英語名, 説明抽出\n(正規表現)};
    G --> H{次の行はメタ情報か？\n(正規表現)};
    H -- Yes --> I{メタ情報抽出};
    I --> H;
    H -- No --> J{タグ情報を辞書に格納};
    J --> E;
    F -- No --> E;
    E --> K{全行処理完了};
    K --> L{タグデータリストをJSONに変換};
    L --> M{JSONファイル出力\n(jp_tags.json)};
    M --> N[終了];
```

## 実装結果

スクリプトは正常に動作し、1576個のタグを抽出してJSONファイルに出力しました。タグ情報には以下が含まれています：

- 日本語タグ名
- タグスラッグ（URLで使用される識別子）
- 英語タグ名（存在する場合）
- 説明文
- カテゴリ情報
- メタ情報（関連タグなど）

## 考慮事項

*   `[[include ...]]` の処理は、ファイルパスの解決と再帰的な読み込みを正確に行う必要があります。
*   正規表現は、英語タグ名の有無に対応できるように設計する必要があります。
*   Wikidotの他の構文（CSSモジュール、タブビューなど）は無視します。
*   文字エンコーディングの問題に対処するため、適切なエラーハンドリングが必要です。

## 出力ファイル

*   `jp_tags.json` - 1576個のタグ情報を含むJSONファイル